{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h1 align=\"center\">Explorar el concepto de Pluralismo utilizando la Ciencia de los Datos: un estudio de caso con el ecosistema mediático de Chile</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div align=\"center\"><i>Autores: Profesores y Estudiantes del Magíster en Informática (Universidad Austral de Chile)</i></div>\n",
    "<div align=\"center\"><i>I semestre 2018</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Objetivo de investigación general</h2>\n",
    "<ul>\n",
    "<li><p>El <b>Pluralismo</b> de los medios es un principio que garantiza que l@s ciudadan@s disponen de una información pólitica e ideólogica diversificada, permitiendoles ejercer su <i>espiritu crítico</i> y su <i>libertad de pensar</i>. Por lo tanto, la Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura (UNESCO) definió el pluralismo de los medios como una condición necesaria para construir la Democracía.</p></li>\n",
    "<li><b>Medir para entender</b>: <i>“When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meagre and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely, in your thoughts, advanced to the stage of science, whatever the matter may be.”</i> - Lord Kelvin (1883)\n",
    "<li><b>Pregunta general:</b> ¿La Informática, más particularmente la Ciencia de los Datos, puede medir el Pluralismo de los medios? ¿Se puede establecer un protocolo computacional para medir y entender varias facetas del Pluralismo de los Medios basandose sobre técnicas de Clustering de datos?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Experimentación de Ciencia de los Datos</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>1. Definir una pregunta de investigación </b></h3>\n",
    "\n",
    "<p><u>Ejemplos de preguntas:</u></p>\n",
    "<ul>\n",
    "<li>En el marco de una temática (<i>Mapuche, Cambio climático, Feminismo, Sexismo, etc.</i>), ¿en cuántos tópicos se dividen los discursos de los medios de prensa? ¿Existen diferencias significativas entre los distintos medios? </li>\n",
    "<li>En el marco de una semana de noticias, ¿en cuántos tópicos se dividen las noticias? ¿Existen diferencias significativas entre los distintos medios?</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Recopilar y preparar los datos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Cargar el dataset de tweets\n",
    "df_feminismo = pd.read_csv('datasets/sophia_cambioclimatico_v2.csv',delimiter=\"|\", header=None)\n",
    "df_feminismo\n",
    "\n",
    "#selección de los mensajes\n",
    "docs = df_feminismo[3].as_matrix()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gauss/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gauss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Explorar los datos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11374.2011378\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: loading working playlists ¿qué vergara podemos frente disminuido acción regional hacer plan laguna peter observar tema enfrentan comité corecc regionales paine reunió medidas mitigación implementación diego internacional terrestres funcionamiento hombre\n",
      "Topic #1: login slide /a br show view href= ago socializer specify section admin x super work in key googleplus linkedin leave caleta algarrobo id consumer api bionoticiascl frutillar quisco coliumo secret\n",
      "Topic #2: minutos dijo energía universidad decisión aumento email agricultura millones mar dos personas medidas especies científicos hoy fenómeno bachelet cómo producción futuro así temperatura plan hacer internacional temperaturas estudio tiempo puede\n",
      "Topic #3: minutos educación piñera sexual hora leyendo libertadores horas videos regional nueva aguas universidad mena riego mar comisión reunión indap estudio presidenciales seguir seminario temas candidatos huracanes científicos isla investigación salud\n",
      "Topic #4: hora wwf edificios luces señal acción marzo campaña adoptando voluntaria promovida sábado conéctate asimismo real ciudades iniciativa aparatos ambiental llamado bosshard torre esenciales icónicos sufrida eléctricos invita prescindibles aludiendo emblemáticos\n",
      "Topic #5: sídney apagarán monumentos luces pirámides edición atenas acrópolis kremlin numerosas egipcias alhambra granada recordará siddarth preocupaban organizadora das únicamente lanzamos movimiento pekín velas tradicional empezando tanzania concierto singapur empezaron simbólico\n",
      "Topic #6: not was to the orrego an on intendente use educación hora url andinas agricultura error innovación this newsletter servicio empresa metropolitana lluvias zona josé plantas aguas potable medios actuar valor\n",
      "Topic #7: servicio boscosos velar mantención deberá recuperación asumir urgencia prevenir marcados catástrofes cuidado contexto wwf incendios diversos desafíos ecosistemas nuevo entusiasmo envío resume facultades recibido creará tareas institución contará conaf puntos\n",
      "Topic #8: newsletter ciudad guioteca hoyxhoy.cl farox elige contáctenos ¿cuándo inmobiliaria soychile.cl entérate quieres emol correos propiedades recibirás sucede autolocal.cl suscríbete anunciar pronto recibir valor condiciones términos futuro sa periodística araucanía gestión\n",
      "Topic #9: vulnerables artículo cómo prevenir maule concepción nobel newsletter velar video posibilidad malo animales cine saben mencionado prevención nueva definitivamente anteriormente continente crítica aquello origen of metropolitana impuestos partes sexual meza\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aplicando Modelos Probabilistas de Tópicos y LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "#stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.2, min_df=7,\n",
    "                                stop_words=stopwords,tokenizer=tokenize_only, ngram_range=(1,1))\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "diccionario= tf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "#Estimación de LDA con Bayes Variacional\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "#Cálculo de índice de ajuste de los datos\n",
    "print(lda.perplexity(tf))\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, diccionario, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
